<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Nidhi AI - Pro Interaction</title>
    
    <style>
        body { 
            margin: 0; padding: 0;
            background-color: #1a1a1a; 
            background-image: linear-gradient(rgba(0,0,0,0.1), rgba(0,0,0,0.1)), url('wallpaper.png');
            background-size: cover;
            background-position: center center;
            background-repeat: no-repeat;
            background-attachment: fixed;
            overflow: hidden; 
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; 
            height: 100vh; width: 100vw;
            cursor: pointer;
            -webkit-tap-highlight-color: transparent;
        }

        canvas { display: block; position: absolute; top: 0; left: 0; z-index: 1; pointer-events: none; }

        #ui-overlay { 
            position: absolute; 
            top: env(safe-area-inset-top, 25px); 
            left: env(safe-area-inset-left, 25px); 
            z-index: 10; 
        }

        #status-indicator { 
            background: rgba(0, 0, 0, 0.5); 
            color: #00ffcc; 
            padding: 8px 16px; 
            border-radius: 20px; 
            font-size: 0.75rem; 
            font-weight: bold; 
            border: 1px solid rgba(0,255,204,0.15); 
            backdrop-filter: blur(8px);
            -webkit-backdrop-filter: blur(8px);
            text-transform: uppercase; 
            letter-spacing: 1.5px; 
            opacity: 0.8;
            transition: opacity 0.3s ease;
        }

        #click-prompt {
            position: absolute; top: 50%; left: 50%; 
            transform: translate(-50%, -50%);
            color: white; font-size: 1.4rem; text-align: center;
            background: rgba(0,0,0,0.6); padding: 25px 50px; border-radius: 40px;
            backdrop-filter: blur(12px); z-index: 100; pointer-events: none;
            transition: opacity 0.8s ease; border: 1px solid rgba(255,255,255,0.1);
            width: 80%; max-width: 400px;
        }

        .hidden { display: none !important; }
    </style>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/6.5.2/browser/pixi.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/dylanNew/live2d/webgl/Live2D/lib/live2d.min.js"></script>
    <script src="https://cubism.live2d.com/sdk-web/cubismcore/live2dcubismcore.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/pixi-live2d-display@0.4.0/dist/index.min.js"></script>
</head>
<body>
    <canvas id="canvas"></canvas>
    <div id="click-prompt">Tap to wake up Sonia</div>
    <div id="ui-overlay"><div id="status-indicator" class="hidden"></div></div>

    <script type="module">
        // Mobile Fix: Force high precision for shaders to prevent black textures
        PIXI.settings.PRECISION_FRAGMENT = PIXI.PRECISION.HIGH;

        const motions = [
            'hello', 'kiss', 'dance', 'sing', 'sniff', 'nod', 'shake_head', 
            'laugh', 'clap', 'wave_both', 'think', 'surprised', 'shrug', 
            'bow', 'hop', 'angry_stomp', 'cry', 'bashful', 'point', 'sleepy', 'heart_gesture'
        ];

        let nidhiModel, recognition;
        let isSpeaking = false;
        let isInitialized = false;

        const app = new PIXI.Application({
            view: document.getElementById('canvas'),
            autoStart: true, 
            resizeTo: window, 
            backgroundAlpha: 0,
            antialias: true,
            resolution: window.devicePixelRatio || 1,
            autoDensity: true,
            powerPreference: 'high-performance'
        });

        const settings = {
            angleX: { smooth: 0.15, min: -30, max: 30 },
            angleY: { smooth: 0.15, min: -30, max: 30 },
            bodyX: { smooth: 0.2, min: -10, max: 10 },
            eyeOpen: { smooth: 0.05, min: 0, max: 1 },
            mouthOpen: { smooth: 0.12, min: 0, max: 1.0 },
            voice: { pitch: 1.1, rate: 1.05 }
        };

        async function initLive2D() {
            try {
                // Load model with autoInteract off to prevent mouse tracking
                nidhiModel = await PIXI.live2d.Live2DModel.from('LiveroiD_A-Y01.model3.json', { autoInteract: false });
                nidhiModel.interactive = false;
                app.stage.addChild(nidhiModel);

                const updateLayout = () => {
                    if (!nidhiModel) return;
                    const isMobile = window.innerWidth < 768;
                    const baseScale = Math.min(window.innerWidth / nidhiModel.width, window.innerHeight / nidhiModel.height);
                    // Adjusted scale for mobile vs desktop centering
                    nidhiModel.scale.set(baseScale * (isMobile ? 3.0 : 1.8)); 
                    nidhiModel.x = window.innerWidth / 2; 
                    nidhiModel.y = window.innerHeight / 2; 
                    nidhiModel.anchor.set(0.5, 0.5);
                };

                updateLayout();
                window.onresize = updateLayout;

                nidhiModel.on('update', () => {
                    if (!nidhiModel.internalModel) return;
                    const core = nidhiModel.internalModel.coreModel;
                    const now = Date.now();

                    // 1. Autonomous Human Swaying (Human-like physics)
                    core.setParameterValueById('ParamBodyAngleX', lerp(core.getParameterValueById('ParamBodyAngleX'), Math.sin(now / 2500) * settings.bodyX.max, settings.bodyX.smooth));
                    core.setParameterValueById('ParamAngleY', lerp(core.getParameterValueById('ParamAngleY'), Math.sin(now / 1500) * 5, settings.angleY.smooth));

                    // 2. Pro Lip Sync during speech
                    if (isSpeaking) {
                        const v = (Math.abs(Math.sin(now / 60)) * 0.5) + (Math.random() * 0.5);
                        core.setParameterValueById('ParamMouthOpenY', lerp(core.getParameterValueById('ParamMouthOpenY'), v, settings.mouthOpen.smooth));
                    } else {
                        core.setParameterValueById('ParamMouthOpenY', lerp(core.getParameterValueById('ParamMouthOpenY'), 0, settings.mouthOpen.smooth));
                    }

                    // 3. Natural Blinking logic
                    const blinkTime = now % 4000;
                    if (blinkTime < 150) { 
                        core.setParameterValueById('ParamEyeLOpen', 0); core.setParameterValueById('ParamEyeROpen', 0);
                    } else {
                        core.setParameterValueById('ParamEyeLOpen', lerp(core.getParameterValueById('ParamEyeLOpen'), 1, settings.eyeOpen.smooth));
                        core.setParameterValueById('ParamEyeROpen', lerp(core.getParameterValueById('ParamEyeROpen'), 1, settings.eyeOpen.smooth));
                    }
                });
            } catch (e) { console.error("Model load error:", e); }
        }

        window.triggerAction = (name) => {
            if (!nidhiModel || !nidhiModel.internalModel) return;
            const index = motions.indexOf(name);
            if (index !== -1) {
                nidhiModel.motion('Motion', index, 3);
            }
        };

        async function getAIResponse(text) {
            isSpeaking = true; if (recognition) recognition.stop();
            document.getElementById('status-indicator').innerText = "THINKING...";
            try {
                const res = await fetch('/api/chat', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ message: text })
                });
                const data = await res.json();
                const reply = data.reply.toLowerCase();
                
                // RESTORED: Trigger physical motions based on AI response keywords
                if (reply.includes("hello") || reply.includes("hi")) triggerAction('hello');
                else if (reply.includes("dance")) triggerAction('dance');
                else if (reply.includes("kiss")) triggerAction('kiss');
                else if (reply.includes("sorry") || reply.includes("sad") || reply.includes("cry")) triggerAction('cry');
                else if (reply.includes("laugh") || reply.includes("haha")) triggerAction('laugh');
                else if (reply.includes("think")) triggerAction('think');
                else if (reply.includes("wow") || reply.includes("surprised")) triggerAction('surprised');
                
                speak(data.reply);
            } catch (err) { isSpeaking = false; if(recognition) try{recognition.start();}catch(e){} }
        }

        function speak(text) {
            const status = document.getElementById('status-indicator');
            status.innerText = "SPEAKING...";
            
            const lowerText = text.toLowerCase();
            let p = settings.voice.pitch, r = settings.voice.rate;

            // RESTORED: Emotional Voice modulation
            if (lowerText.includes("haha") || lowerText.includes("happy") || lowerText.includes("yay") || lowerText.includes("!")) {
                p = 1.3; r = 1.15; // Higher/Faster for excitement
            } else if (lowerText.includes("sorry") || lowerText.includes("sad") || lowerText.includes("unfortunate")) {
                p = 0.9; r = 0.85; // Lower/Slower for sympathy
            } else if (lowerText.includes("?") || lowerText.includes("really") || lowerText.includes("wow")) {
                p = 1.25; r = 1.05; // Alert for curiosity
            }

            let clean = text.replace(/\*[^*]*\*/g, '').replace(/[^\x00-\x7F\u0900-\u097F\s]/g, '').trim();
            const utterance = new SpeechSynthesisUtterance(clean);
            const voices = window.speechSynthesis.getVoices();
            const isHindi = /[\u0900-\u097F]/.test(clean);
            
            let v = voices.find(v => (isHindi ? v.lang.startsWith('hi') : v.name.includes("Sonia")) && v.name.includes("Natural"));
            if (!v) v = voices.find(v => v.name.includes("Aria") && v.name.includes("Natural"));
            if (!v) v = voices.find(v => v.name.includes("Female"));
            
            if (v) utterance.voice = v;
            utterance.pitch = p; 
            utterance.rate = r;
            
            utterance.onstart = () => { isSpeaking = true; };
            utterance.onend = () => { 
                isSpeaking = false; 
                if(nidhiModel) nidhiModel.internalModel.coreModel.setParameterValueById('ParamMouthOpenY', 0);
                status.innerText = "LISTENING..."; 
                try{recognition.start();}catch(e){} 
            };
            window.speechSynthesis.speak(utterance);
        }

        function setupVoice() {
            const Speech = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (!Speech) return;
            recognition = new Speech();
            recognition.continuous = true;
            recognition.onresult = async (e) => {
                const text = e.results[e.results.length - 1][0].transcript.trim();
                await getAIResponse(text);
            };
            recognition.onend = () => { if (!isSpeaking) try { recognition.start(); } catch(err){} };
            recognition.start();
        }

        const lerp = (s, e, a) => (1 - a) * s + a * e;
        const init = async () => {
            if (isInitialized) return;
            const prompt = document.getElementById('click-prompt');
            if (prompt) { prompt.style.opacity = "0"; setTimeout(() => prompt.remove(), 800); }
            const status = document.getElementById('status-indicator');
            status.classList.remove('hidden');
            status.innerText = "LISTENING...";
            await initLive2D(); setupVoice(); isInitialized = true;
        };

        // Interaction listeners for mobile and desktop
        document.body.addEventListener('mousedown', init, { once: true });
        document.body.addEventListener('touchstart', init, { once: true });
        
        // Ensure voices are loaded for selection
        window.speechSynthesis.getVoices();
        if (speechSynthesis.onvoiceschanged !== undefined) {
            speechSynthesis.onvoiceschanged = () => window.speechSynthesis.getVoices();
        }
    </script>
</body>
</html>
