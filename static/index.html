<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Nidhi AI - Voice & Vision</title>
    
    <!-- 1. PixiJS (v7) -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.3.3/pixi.min.js"></script>
    
    <!-- 2. Cubism 2.1 Runtime (CRITICAL FIX: Solves the 'Could not find Cubism 2 runtime' error) -->
    <script src="https://cdn.jsdelivr.net/gh/dazzal/pixi-live2d-display/dist/live2d.min.js"></script>
    
    <!-- 3. Cubism 4 Core (Required for your Model3.json) -->
    <script src="https://cubism.live2d.com/sdk-web/cubismcore/live2dcubismcore.min.js"></script>
    
    <!-- 4. Pixi Live2D Display Plugin (Loads after the Runtimes) -->
    <script src="https://cdn.jsdelivr.net/npm/pixi-live2d-display@0.4.0/dist/index.min.js"></script>

    <!-- 5. MediaPipe Face Mesh (For Tracking) -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js" crossorigin="anonymous"></script>
    
    <style>
        body { margin: 0; background-color: #1a1a1a; overflow: hidden; font-family: sans-serif; }
        canvas { display: block; }
        
        #input-video {
            position: absolute; opacity: 0; pointer-events: none; z-index: -1;
        }

        #ui-overlay {
            position: absolute; bottom: 50px; left: 50%; transform: translateX(-50%);
            display: flex; flex-direction: column; align-items: center; gap: 20px; z-index: 10;
        }
        
        #status-indicator {
            background: rgba(0, 0, 0, 0.6); color: #00ffcc; padding: 10px 20px;
            border-radius: 25px; font-weight: bold; border: 1px solid #00ffcc;
        }

        #subtitle {
            background: rgba(0, 0, 0, 0.8); color: #fff; padding: 15px 30px;
            border-radius: 15px; text-align: center; font-size: 1.2em; max-width: 80vw;
        }
        
        #start-btn { 
            padding: 18px 45px; border-radius: 35px; border: none; 
            background: linear-gradient(135deg, #00d2ff, #3a7bd5); 
            color: white; font-weight: bold; font-size: 1.2em; cursor: pointer;
            box-shadow: 0 4px 15px rgba(0,210,255,0.4);
        }
        .hidden { display: none !important; }
        #error-msg { 
            color: #ff5555; background: rgba(0,0,0,0.85); padding: 15px; 
            border-radius: 10px; display: none; max-width: 80vw; text-align: center;
        }
    </style>
</head>
<body>
    <video id="input-video"></video>
    <canvas id="canvas"></canvas>
    
    <div id="ui-overlay">
        <div id="error-msg"></div>
        <div id="status-indicator" class="hidden">System Ready</div>
        <div id="subtitle" class="hidden">Listening...</div>
        <button id="start-btn" onclick="initApp()">Start Interaction</button>
    </div>

    <script>
        let nidhiModel, faceMesh, camera, recognition;
        let isSpeaking = false;

        // Initialize Pixi Application
        const app = new PIXI.Application({
            view: document.getElementById('canvas'),
            autoStart: true,
            resizeTo: window,
            backgroundColor: 0x1a1a1a
        });

        /**
         * Enhanced check for PIXI Live2D registration.
         * Ensures all layers of the library are ready before we attempt to load the model.
         */
        async function waitForLive2D() {
            let attempts = 0;
            while (attempts < 100) {
                // Check for both the PIXI object AND the live2d plugin extension
                if (window.PIXI && PIXI.live2d && PIXI.live2d.Live2DModel) {
                    console.log("Live2D Libraries detected and ready.");
                    return true;
                }
                attempts++;
                await new Promise(r => setTimeout(r, 100));
            }
            throw new Error("Live2D Runtime (Cubism) or Pixi Plugin failed to initialize. Check if external scripts are blocked or slow.");
        }

        // Robust Model Loading
        (async function load() {
            try {
                await waitForLive2D();

                console.log("Loading Model: LiveroiD_A-Y01.model3.json");
                
                // Load the model from the static directory
                nidhiModel = await PIXI.live2d.Live2DModel.from('LiveroiD_A-Y01.model3.json');
                
                app.stage.addChild(nidhiModel);

                // Positioning and Scaling logic
                const scale = Math.min(innerWidth / nidhiModel.width, innerHeight / nidhiModel.height) * 0.85;
                nidhiModel.scale.set(scale);
                nidhiModel.x = innerWidth / 2;
                nidhiModel.y = innerHeight / 2;
                nidhiModel.anchor.set(0.5, 0.5);
                
                window.onresize = () => {
                    if (nidhiModel) {
                        nidhiModel.x = innerWidth / 2;
                        nidhiModel.y = innerHeight / 2;
                    }
                };

                console.log("Nidhi Model successfully loaded!");

            } catch (error) {
                console.error("Critical Model Error:", error);
                const errBox = document.getElementById('error-msg');
                errBox.style.display = 'block';
                errBox.innerHTML = `<strong>Error:</strong> ${error.message}<br><small>Ensure 'LiveroiD_A-Y01.model3.json' and textures are correctly uploaded to the 'static' folder.</small>`;
            }
        })();

        // Face Tracking Results Handler
        function onResults(results) {
            if (!nidhiModel || !nidhiModel.internalModel || !results.multiFaceLandmarks || results.multiFaceLandmarks.length === 0) return;
            
            const landmarks = results.multiFaceLandmarks[0];
            const nose = landmarks[1];
            const leftEar = landmarks[234];
            const rightEar = landmarks[454];
            const top = landmarks[10];
            const chin = landmarks[152];

            // Head rotation mapping based on face mesh landmarks
            const yaw = ((Math.abs(nose.x - leftEar.x) - Math.abs(nose.x - rightEar.x)) / 0.1) * 15;
            const pitch = ((Math.abs(nose.y - top.y) / Math.abs(top.y - chin.y)) - 0.4) * 100;
            const roll = (leftEar.y - rightEar.y) * 150;

            const core = nidhiModel.internalModel.coreModel;
            // Update head angle parameters
            core.setParameterValueById('ParamAngleX', yaw);
            core.setParameterValueById('ParamAngleY', pitch);
            core.setParameterValueById('ParamAngleZ', roll);
            
            // Sync eyeballs with head direction for natural look
            core.setParameterValueById('ParamEyeBallX', yaw / 30);
            core.setParameterValueById('ParamEyeBallY', pitch / 30);
        }

        async function initApp() {
            // Remove start button and show UI
            document.getElementById('start-btn').classList.add('hidden');
            document.getElementById('status-indicator').classList.remove('hidden');
            document.getElementById('subtitle').classList.remove('hidden');

            try {
                // Initialize FaceMesh Tracking (MediaPipe)
                faceMesh = new FaceMesh({locateFile: (f) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${f}`});
                faceMesh.setOptions({maxNumFaces: 1, refineLandmarks: true, minDetectionConfidence: 0.5});
                faceMesh.onResults(onResults);

                camera = new Camera(document.getElementById('input-video'), {
                    onFrame: async () => { await faceMesh.send({image: document.getElementById('input-video')}); },
                    width: 640, height: 480
                });
                await camera.start();
                
                // Initialize Voice Loop
                setupVoice();

            } catch (e) {
                const errBox = document.getElementById('error-msg');
                errBox.style.display = 'block';
                errBox.innerText = "Camera/Tracking Error: " + e.message;
            }
        }

        function setupVoice() {
            const Speech = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (!Speech) {
                alert("Speech recognition not supported in this browser. Please use Chrome.");
                return;
            }
            
            recognition = new Speech();
            recognition.lang = 'en-US'; 
            recognition.continuous = false;
            
            recognition.onstart = () => {
                if(!isSpeaking) document.getElementById('status-indicator').innerText = "LISTENING...";
            };
            
            recognition.onresult = async (e) => {
                const text = e.results[0][0].transcript;
                document.getElementById('subtitle').innerText = `You: ${text}`;
                await getAIResponse(text);
            };
            
            recognition.onerror = (e) => {
                if (!isSpeaking && e.error !== 'aborted') {
                   setTimeout(() => { try { recognition.start(); } catch(err){} }, 1000); 
                }
            };

            recognition.onend = () => { 
                if (!isSpeaking) {
                    try { recognition.start(); } catch(err){}
                }
            };
            
            try { recognition.start(); } catch(err){}
        }

        async function getAIResponse(text) {
            isSpeaking = true;
            recognition.stop();
            document.getElementById('status-indicator').innerText = "THINKING...";
            
            try {
                const res = await fetch('/api/chat', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ message: text })
                });
                
                const data = await res.json();
                document.getElementById('subtitle').innerText = data.reply;
                speak(data.reply);
            } catch (err) {
                console.error("Gemini API Error:", err);
                isSpeaking = false;
                recognition.start();
            }
        }

        function speak(text) {
            document.getElementById('status-indicator').innerText = "NIDHI SPEAKING...";
            const utterance = new SpeechSynthesisUtterance(text);
            const voices = speechSynthesis.getVoices();
            const isHindi = /[\u0900-\u097F]/.test(text);

            // Prioritize Female Voices as requested (Sonia/Aria/Hindi Natural)
            let v = voices.find(v => v.name.includes("Sonia") || v.name.includes("Aria"));
            if (isHindi) v = voices.find(v => v.lang.includes("hi") && v.name.includes("Female")) || voices.find(v => v.lang.includes("hi"));
            if (!v) v = voices.find(v => v.name.includes("Female") && v.lang.includes("en"));
            
            if (v) utterance.voice = v;
            utterance.pitch = 1.1;
            
            // Lip Sync Manual Animation Loop
            let sync = setInterval(() => {
                if(nidhiModel && nidhiModel.internalModel) {
                    nidhiModel.internalModel.coreModel.setParameterValueById('ParamMouthOpenY', Math.random());
                }
            }, 100);

            utterance.onend = () => {
                clearInterval(sync);
                if(nidhiModel) nidhiModel.internalModel.coreModel.setParameterValueById('ParamMouthOpenY', 0);
                isSpeaking = false;
                try { recognition.start(); } catch(err){}
            };
            
            speechSynthesis.speak(utterance);
        }
    </script>
</body>
</html>
